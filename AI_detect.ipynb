{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory containing the image folders\n",
    "base_path = \"/Users/mrbinit/Downloads/archive/\"\n",
    "\n",
    "# List to store all image file paths and corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Function to recursively find all image files in a directory and assign labels\n",
    "def find_image_files(directory, label):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Check for image file formats\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "                labels.append(label)  # Assign label to the corresponding image\n",
    "\n",
    "# Find image files in the 'fake-v2' folder\n",
    "fake_v2_path = os.path.join(base_path, \"fakeV2/fake-v2\")\n",
    "find_image_files(fake_v2_path, label=1)  # Assign label 1 for AI-generated images\n",
    "\n",
    "# Find image files in the 'real' folder\n",
    "real_path = os.path.join(base_path, \"real\")\n",
    "find_image_files(real_path, label=0)  # Assign label 0 for hand-made images\n",
    "\n",
    "# Shuffle the image paths and labels in parallel\n",
    "combined_data = list(zip(image_paths, labels))\n",
    "random.shuffle(combined_data)\n",
    "image_paths, labels = zip(*combined_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "# Define the percentage of data  \n",
    "data_percentage = 5\n",
    "num_samples = int(len(image_paths) * (data_percentage / 100))\n",
    "\n",
    "# Use a subset of image paths and labels\n",
    "sampled_image_paths = image_paths[:num_samples]\n",
    "sampled_labels = labels[:num_samples]\n",
    "\n",
    "# Load and process the sampled images\n",
    "for img_path, label in zip(sampled_image_paths, sampled_labels):\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at '/Users/mrbinit/Downloads/archive/real/0uqpnep6pc6a1.jpg' has dimensions: 1440x1440 pixels with 3 channels\n"
     ]
    }
   ],
   "source": [
    "height, width, channels = img.shape\n",
    "print(f\"Image at '{img_path}' has dimensions: {width}x{height} pixels with {channels} channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the entire dataset is: 21635\n",
      "the size of the entire dataset after  1081\n"
     ]
    }
   ],
   "source": [
    "prev_dataset_size = len(image_paths)\n",
    "dataset_size = len(sampled_image_paths)\n",
    "print('The size of the entire dataset is:', prev_dataset_size)\n",
    "print('the size of the entire dataset after ', dataset_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the entire dataset was huge and my computer is not capable to handle such a big dataset so I decided to use 20% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot histogram of pixel intensities for a sample image\n",
    "def plot_histogram(sampled_image_paths):\n",
    "    img = cv2.imread(sampled_image_paths, cv2.IMREAD_GRAYSCALE) #IMREADGREYSCALE reads the image in grey scale\n",
    "    if img is not None:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.hist(img.ravel(), bins = 256, range =(0, 255), density = True, color = 'black', alpha = 0.7)\n",
    "        plt.xlabel('Pixel intensity')\n",
    "        plt.ylabel('Normalized frequency')\n",
    "        plt.title('Histogram of pixel intensities')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'Failed to read image at {sampled_image_paths}')\n",
    "\n",
    "# Plot histogram for a sample image\n",
    "sample_image_path = sampled_image_paths[0]\n",
    "plot_histogram(sample_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 1730\n",
      "Size of test set: 216\n",
      "Size of validation set: 217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train, test, and validation sets using sklearn\n",
    "train_paths, test_val_paths, train_labels, test_val_labels = train_test_split(\n",
    "    sampled_image_paths, sampled_labels, test_size=0.2, stratify=sampled_labels, random_state=42)\n",
    "\n",
    "test_paths, val_paths, test_labels, val_labels = train_test_split(\n",
    "    test_val_paths, test_val_labels, test_size=0.5, stratify=test_val_labels, random_state=42)\n",
    "\n",
    "# Display sizes of train, test, and validation sets\n",
    "print('Size of train set:', len(train_paths))\n",
    "print('Size of test set:', len(test_paths))\n",
    "print('Size of validation set:', len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define image augmentation layers\n",
    "img_augmentation_layers = [\n",
    "    tf.keras.layers.RandomRotation(factor=0.15),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "    tf.keras.layers.RandomFlip(),\n",
    "    tf.keras.layers.RandomContrast(factor=0.1),\n",
    "]\n",
    "\n",
    "def apply_image_augmentation(image):\n",
    "    # Convert image to TensorFlow tensor\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    # Apply each augmentation layer sequentially\n",
    "    for layer in img_augmentation_layers:\n",
    "        image = layer(image)\n",
    "    # Convert back to NumPy array\n",
    "    augmented_image = image.numpy()\n",
    "    return augmented_image\n",
    "\n",
    "def load_and_preprocess_images(image_paths, labels, input_size=224, augment=True):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        # Read and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (input_size, input_size))\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        if augment:\n",
    "            augmented_image = apply_image_augmentation(image)\n",
    "            images.append(augmented_image)\n",
    "        else:\n",
    "            images.append(image)\n",
    "\n",
    "    images = np.array(images)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/q65t_5bs3653ct73dh3s10w00000gn/T/ipykernel_41427/2362135105.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train, y_train = np.array([item[0] for item in train_data]), np.array([item[1] for item in train_data])\n",
      "/var/folders/_7/q65t_5bs3653ct73dh3s10w00000gn/T/ipykernel_41427/2362135105.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val, y_val = np.array([item[0] for item in val_data]), np.array([item[1] for item in val_data])\n",
      "/var/folders/_7/q65t_5bs3653ct73dh3s10w00000gn/T/ipykernel_41427/2362135105.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test, y_test = np.array([item[0] for item in test_data]), np.array([item[1] for item in test_data])\n"
     ]
    }
   ],
   "source": [
    "from efficientnet.tfkeras import EfficientNetB4\n",
    "\n",
    "# Define input size for EfficientNetB0\n",
    "input_size = 224  # EfficientNetB0 input size\n",
    "\n",
    "# Preprocess the training data\n",
    "train_data = load_and_preprocess_images(train_paths, train_labels, input_size, augment=True)\n",
    "val_data = load_and_preprocess_images(val_paths, val_labels, input_size, augment=False)\n",
    "test_data = load_and_preprocess_images(test_paths, test_labels, input_size, augment= False)\n",
    "\n",
    "# # Shuffle training data\n",
    "# train_data = shuffle(train_data)\n",
    "\n",
    "# Convert preprocessed data to numpy arrays\n",
    "X_train, y_train = np.array([item[0] for item in train_data]), np.array([item[1] for item in train_data])\n",
    "X_val, y_val = np.array([item[0] for item in val_data]), np.array([item[1] for item in val_data])\n",
    "X_test, y_test = np.array([item[0] for item in test_data]), np.array([item[1] for item in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "\u001b[1m16804768/16804768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Build EfficientNetB4 model\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(input_size, input_size, 3))\n",
    "base_model.trainable = False\n",
    "# Add classification head\n",
    "# dropout_rate = 0.2\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    # tf.keras.layers.Dropout(dropout_rate),\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model (optional)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# test_images, test_labels = load_images_from_folder(test_folder_path)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# model.evaluate(test_images, test_labels)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py:553\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         )\n\u001b[1;32m    559\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[1;32m    560\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load images and labels from a folder\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\"):  # Assuming images are saved in JPG format\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            image = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename.split('_')[0])  # Assuming filename format: label_number.jpg\n",
    "\n",
    "    # Convert labels to categorical format using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "    return np.array(images), labels\n",
    "\n",
    "# Load images and labels from the processed folder\n",
    "folder_path = '/Users/mrbinit/Downloads/preprocessed'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')  # Assuming 2 classes (fake and real)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# test_images, test_labels = load_images_from_folder(test_folder_path)\n",
    "# model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate the model (optional)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# test_images, test_labels = load_images_from_folder(test_folder_path)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# model.evaluate(test_images, test_labels)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py:553\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         )\n\u001b[1;32m    559\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[1;32m    560\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load images and labels from a folder\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\"):  # Assuming images are saved in JPG format\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            image = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename.split('_')[0])  # Assuming filename format: label_number.jpg\n",
    "\n",
    "    # Convert labels to categorical format using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "    return np.array(images), labels\n",
    "\n",
    "# Load images and labels from the processed folder\n",
    "folder_path = '/Users/mrbinit/Downloads/preprocessed'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # Assuming 2 classes (fake and real)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# test_images, test_labels = load_images_from_folder(test_folder_path)\n",
    "# model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
